{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "pip install transformers # not needed in Colab\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Natural Language Processing (NLP) Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification\n",
        "This is often the \"hello, world\" of transformers. The pipeline classifies a piece of text into a set of predefined categories. The most common use is sentiment analysis, which determines if a text is positive, negative, or neutral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c38aeca5076e43379a0816625a3b0fc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f4395cc74e34e319a15d98534b6e4d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4d79595bc4747788feb82115aaeed49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8afe1b0b68464418a056f7bf0ede894b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.999752938747406}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"This is not okay thing to do.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we move ahead, let's understand the what is printed in the console."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Default Model Loaded**: Since we didn't specify a particular model, so the system defaulted to loading **`distilbert-base-uncased-finetuned-sst-2-english`**. This is a popular model for sentiment analysis.\n",
        "* **Authentication Warning**: We're seeing a `UserWarning` because we haven't authenticated with Hugging Face using a secret token (`HF_TOKEN`). While not required for public models, it's recommended practice, especially for private models or for tracking usage.\n",
        "* **Model Download**: The green progress bars show the successful download of the model's components. This includes the model's architecture (`config.json`), its trained weights (`model.safetensors`), and the tokenizer files (`tokenizer_config.json`, `vocab.txt`) which prepare text for the model.\n",
        "* **Hardware Allocation**: The final line, `Device set to use cpu`, indicates that the model will run on the Central Processing Unit (CPU) of the machine. For larger models, a GPU or TPU would provide a significant speed-up.\n",
        "* **Model Prediciton**: The output is a list containing a dictionary with the predicted label and its corresponding confidence score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pipeline takes a starting prompt and generates subsequent text, making it useful for creative writing, code completion, or chatbot responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "435a9853664640d29e35fe6a59d3aced"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d9810e82d3649a7963dbce4dac58b58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a082acc8d39f47f08057076faaff7463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "880b2ba58c8c437b84e43df0515e388c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47d670a659f74f0787579ed8caae04ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "933e08e5a8064fc5b6b76bb9e760652e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35021d0585604f49b2c8300acfd61e14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'In a world where AI is king, the U.S. government plans to use artificial intelligence to build an AI system that will use human intelligence to drive cars and airplanes. The program would be run by the National Highway Traffic Safety Administration, which would be led by a U.S. Department of Transportation employee.\\n\\nThe program would involve using information about drivers, such as the amount of time they\\'ve been driving, to get a driver to stop, and then to complete a short-term test to see how much human intelligence would be able to help.\\n\\nThe program would also use computers to help engineers build software that could help police and firefighters identify people who might be in serious trouble based on a specific type of criminal or illegal activity.\\n\\nAdvertisement\\n\\nThe program would also be used to help people who are at a risk of being arrested for a violent crime with a warrant or other form of identification, which would help people who have been charged with a violent crime with a warrant.\\n\\n\"This is one of the most important steps in our efforts to protect the privacy and security of all Americans and to ensure that our nation\\'s economy is not undermined by the Internet,\" said Tom E. Wheeler, Chairman of the Federal Communications Commission. \"In this regard, the Obama Administration is committed to'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "result = generator(\"In a world where AI is king,\", max_length=30, num_return_sequences=1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering\n",
        "This pipeline extracts an answer to a question from a given context paragraph. It does not generate new text but identifies the span of text within the context that best answers the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c1d3ca0889444f4a9746fe3ef66adc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d018b9768b441b5bd21e121df5ce2f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fc37c404f54efab14b9182e36419b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58d1943912eb4e5e980a4cf2f8ca191c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abb91491633c4821b2a0800250738b02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.5052773952484131, 'start': 55, 'end': 75, 'answer': 'models for inference'}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "context = \"The Hugging Face pipeline abstraction simplifies using models for inference.\"\n",
        "question = \"What does the pipeline abstraction simplify?\"\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summarization\n",
        "This pipeline condenses a long piece of text into a shorter, concise summary, capturing the main points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be015522fbd242baa6970d63565c270d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "797e870b7c8f4e25a2ddf25bfab8226f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9278517d771b464e835b9ed29fd7cb60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63f7ff8c53444adaadb5fd85cff8c394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96e07377b37b49a18c68810f7660d4f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a7377613e6f475799d63fefebd53177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': ' The red fox boasts the widest geographical distribution of any member of the order Carnivora . Its range extends across the entire Northern Hemisphere, from the Arctic Circle to North Africa, North America, and Eurasia . In urban settings, fox'}]\n"
          ]
        }
      ],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "article = \"\"\"The red fox, a creature of cunning and adaptability, is a common sight in both rural and urban environments. Despite its name, this mammal can be found in a variety of colors, including silver, black, and other variations. The most iconic morph, however, presents a fiery red coat that contrasts sharply with its white underbelly, black-tipped ears, and dark legs. Its long, bushy tail, often tipped with a distinctive white blaze, serves multiple purposes: as a warm covering in cold weather, a signal flag to other foxes, and a counterbalance for agile maneuvering.\n",
        "Scientifically known as Vulpes vulpes, the red fox boasts the widest geographical distribution of any member of the order Carnivora. Its range extends across the entire Northern Hemisphere, from the Arctic Circle to North Africa, North America, and Eurasia. This remarkable success is a testament to its incredible ability to thrive in a vast array of habitats, from dense forests and open grasslands to harsh deserts and even the heart of bustling cities. In recent decades, red foxes have become increasingly common in suburban and urban areas, where they have proven adept at navigating human-dominated landscapes and exploiting new food sources.\n",
        "The diet of the red fox is as varied as its habitat. An opportunistic omnivore, it preys primarily on small rodents like mice and voles, as well as rabbits, squirrels, and birds. However, it will readily supplement its diet with fruits, berries, insects, and carrion. In urban settings, foxes are notorious for scavenging from garbage cans and compost heaps, a behavior that highlights their resourcefulness. Their hunting techniques are a marvel of instinct and intelligence. With keen hearing, they can locate prey moving underground and will often perform a characteristic high pounce, leaping into the air to surprise and pin down their meal.\n",
        "Socially, red foxes are generally solitary hunters, though they live in small family groups in underground dens, often referred to as \"earths.\" These dens can be complex, with multiple entrances and chambers, and are sometimes inherited and expanded over generations. A dominant pair, the dog fox and the vixen, will mate for life and raise a litter of kits each spring. The kits, born blind and deaf, are cared for by both parents and sometimes by older siblings from a previous litter who act as \"helpers.\" By autumn, the young foxes will have dispersed to establish their own territories.\n",
        "The relationship between humans and red foxes is complex and multifaceted. In many cultures, the fox is a symbol of cunning and trickery, a prominent figure in folklore and fables. While they can be a nuisance to farmers due to occasional predation on poultry, they also play a crucial role in controlling populations of agricultural pests. Their growing presence in cities has led to a mixture of delight and concern, with many people enjoying the sight of these wild creatures while others worry about potential conflicts. Regardless of human perception, the red fox continues to demonstrate a remarkable resilience, a testament to its evolutionary prowess and its ability to adapt to a changing world.\n",
        "\"\"\"\n",
        "result = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation\n",
        "The pipeline can translate text from a source language to a target language. The task identifier follows the format `translation_{src}_to_{tgt}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7499fe73e104be281c25f4f47c2650a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f249306bbb9d4a3a8241a5dc1375b10f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4ece98f3de3420e9baa6efa2454de40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aa8742af7ab4698a0fa664e69cdedbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67eb6db10e984acbb8c4042ba7cc7d86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'Les transformateurs sont puissants.'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_fr\")\n",
        "result = translator(\"Transformers are powerful.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Classification (Named Entity Recognition - NER)\n",
        "This pipeline, identified by \"ner\" or \"token-classification\", classifies each token in a sentence into a category, such as a person (`PER`), organization (`ORG`), or location (`LOC`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72bbdac04a9142838bbf7cf2def81606"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a589ff99aa844918a213bcc9cff44c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b38f0db1fba4cb4add12831a9a2532b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea3eb0da0e874a1b8a839e0e5ea143dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'I-ORG', 'score': np.float32(0.9992662), 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}, {'entity': 'I-ORG', 'score': np.float32(0.9808883), 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}, {'entity': 'I-ORG', 'score': np.float32(0.99536246), 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}, {'entity': 'I-ORG', 'score': np.float32(0.9993383), 'index': 4, 'word': 'Inc', 'start': 13, 'end': 16}, {'entity': 'I-LOC', 'score': np.float32(0.9990269), 'index': 11, 'word': 'New', 'start': 40, 'end': 43}, {'entity': 'I-LOC', 'score': np.float32(0.9988483), 'index': 12, 'word': 'York', 'start': 44, 'end': 48}, {'entity': 'I-LOC', 'score': np.float32(0.9991774), 'index': 13, 'word': 'City', 'start': 49, 'end': 53}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "ner_pipeline = pipeline(\"ner\")\n",
        "result = ner_pipeline(\"Hugging Face Inc. is a company based in New York City.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fill-Mask\n",
        "Given a text with a masked token (e.g., <mask>), this pipeline predicts the most likely words to fill that mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aa7ec1f1fc84b70886f754ef9a6eb24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41b6915790374384b44ddbf5a6035e3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f1668fee4b8441d89a53c3079c7754f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1f40739bb784be2b62338f65ff24b99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72108b5c1b1743f4b81ee4aabef5a86e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17b0018640bd4f72aac3ff2dcc7280e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.5139005184173584, 'token': 1005, 'token_str': ' Europe', 'sequence': 'The capital of Europe is Paris.'}, {'score': 0.323322057723999, 'token': 1470, 'token_str': ' France', 'sequence': 'The capital of France is Paris.'}, {'score': 0.016538726165890694, 'token': 730, 'token_str': ' America', 'sequence': 'The capital of America is Paris.'}, {'score': 0.014298939146101475, 'token': 31478, 'token_str': ' globalization', 'sequence': 'The capital of globalization is Paris.'}, {'score': 0.005346512887626886, 'token': 2201, 'token_str': ' Paris', 'sequence': 'The capital of Paris is Paris.'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "result = unmasker(\"The capital of <mask> is Paris.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-Shot Classification\n",
        "This is one of the most powerful and flexible pipelines. It allows you to classify text using any set of candidate labels you provide on the fly, without the model having been explicitly trained on those specific labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4c2f5e9361c4788b50d662602ffbd9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf67fc629eed42359d22a716a42bd2b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df0856a6baf24534bfe426d126c0e6b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27419960dd89446fac219234a8bedcfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66cb6acd3aa74d21bb5d132d1c0197fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88f9ed6881b3430d8921fc6b4938fd9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'Who are you voting for in 2024?', 'labels': ['politics', 'technology', 'business'], 'scores': [0.9286003708839417, 0.03690174221992493, 0.034497883170843124]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "sequence = \"Who are you voting for in 2024?\"\n",
        "candidate_labels = [\"politics\", \"business\", \"technology\"]\n",
        "result = zero_shot_classifier(sequence, candidate_labels)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computer Vision, Audio, and Multimodal Pipelines\n",
        "\n",
        "The pipeline abstraction extends seamlessly to modalities beyond text, demonstrating its versatility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code to download cat image - IGNORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "! pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "image_url = \"https://commons.wikimedia.org/wiki/Special:FilePath/Cat_August_2010-4.jpg\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; MyBot/1.0; +https://example.com)\"\n",
        "}\n",
        "\n",
        "print(\"Downloading image...\")\n",
        "response = requests.get(image_url, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "with open(\"cat.jpg\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Success! Image saved as cat.jpg\")\n"
      ],
      "metadata": {},
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading image...\n",
            "Success! Image saved as cat.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Classification\n",
        "Assigns a label to an entire image from a set of predefined classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 3f49326 (https://huggingface.co/google/vit-base-patch16-224).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5615f037c5024278a295562d14eb0068"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3592e5b7e15b4390aba5a915ab6f03a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "701c46ca77f243d6a84f6da6090952b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'Egyptian cat', 'score': 0.8821499347686768}, {'label': 'tabby, tabby cat', 'score': 0.07465515285730362}, {'label': 'tiger cat', 'score': 0.03799121454358101}, {'label': 'lynx, catamount', 'score': 0.0009538873564451933}, {'label': 'tiger, Panthera tigris', 'score': 0.00010449585533933714}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "image_classifier = pipeline(\"image-classification\")\n",
        "result = image_classifier(\"/content/cat.jpg\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Object Detection\n",
        "Identifies multiple objects within an image and returns their class labels along with bounding box coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/detr-resnet-50 and revision 1d5f47b (https://huggingface.co/facebook/detr-resnet-50).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82bb234dfe474af0a394f6bcd7ee9f5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaee4f9d17cd489caf4b2083a5636d5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95d251f91e474b8d8c1f37c0541eff2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer1.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer2.3.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.3.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.4.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer3.5.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.1.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.conv3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:2446: UserWarning: for layer4.2.bn3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/290 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37544a97786d4373b80194067073c2e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.9987272620201111, 'label': 'cat', 'box': {'xmin': 176, 'ymin': 456, 'xmax': 3496, 'ymax': 1946}}, {'score': 0.5731998085975647, 'label': 'car', 'box': {'xmin': 156, 'ymin': 1329, 'xmax': 3633, 'ymax': 2202}}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "result = object_detector(\"/content/cat.jpg\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image Segmentation\n",
        "Classifies each pixel in an image into a category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/detr-resnet-50-panoptic and revision d53b52a (https://huggingface.co/facebook/detr-resnet-50-panoptic).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb7b8d0a5b8d4a5884e51ff4085a7894"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/172M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0cbcd359e8c4df6aa3c549106c7e17e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/detr-resnet-50-panoptic were not used when initializing DetrForSegmentation: ['detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing DetrForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DetrForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2cae70451294c25962ea6b7517ccfa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/172M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c9cb613316d4c83a84e2a9923c229ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`label_ids_to_fuse` unset. No instance will be fused.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.998772, 'label': 'LABEL_199', 'mask': <PIL.Image.Image image mode=L size=3640x2226 at 0x7C458CE63B30>}, {'score': 0.999483, 'label': 'cat', 'mask': <PIL.Image.Image image mode=L size=3640x2226 at 0x7C458CE60290>}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "object_detector = pipeline(\"image-segmentation\")\n",
        "result = object_detector(\"/content/cat.jpg\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: LABEL_199, Score: 0.9988\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: cat, Score: 0.9995\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for item in result:\n",
        "    if 'mask' in item and isinstance(item['mask'], Image.Image):\n",
        "        print(f\"Label: {item.get('label', 'N/A')}, Score: {item.get('score', 'N/A'):.4f}\")\n",
        "        plt.imshow(item['mask'])\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Tasks Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|Modality       |Task Identifier             |Task Aliases      |Description                                        |Expected Input(s)                    |Sample Output Structure                                      |\n",
        "|---------------|----------------------------|------------------|---------------------------------------------------|-------------------------------------|-------------------------------------------------------------|\n",
        "|NLP            |text-classification         |sentiment-analysis|Classifies a sequence into a set of labels.        |str or List[str]                     |[{'label': str, 'score': float}]                             |\n",
        "|NLP            |token-classification        |ner               |Classifies each token in a sequence.               |str or List[str]                     |[{'entity': str, 'score': float, 'word': str,...}]           |\n",
        "|NLP            |question-answering          |                  |Extracts an answer span from a context.            |Dict{'question': str, 'context': str}|{'score': float, 'start': int, 'end': int, 'answer': str}    |\n",
        "|NLP            |fill-mask                   |                  |Predicts a masked token in a sequence.             |str or List[str] with a mask token   |[{'score': float, 'token_str': str,...}]                     |\n",
        "|NLP            |summarization               |                  |Generates a concise summary of a long text.        |str or List[str]                     |[{'summary_text': str}]                                      |\n",
        "|NLP            |translation_xx_to_yy        |                  |Translates text from language xx to yy.            |str or List[str]                     |[{'translation_text': str}]                                  |\n",
        "|NLP            |text-generation             |                  |Generates text following a prompt.                 |str or List[str]                     |[{'generated_text': str}]                                    |\n",
        "|NLP            |zero-shot-classification    |                  |Classifies a sequence using arbitrary labels.      |str, candidate_labels=List[str]      |{'sequence': str, 'labels': List[str], 'scores': List[float]}|\n",
        "|NLP            |feature-extraction          |                  |Converts text into a dense vector representation.  |str or List[str]                     |List[List[float]] or np.ndarray                              |\n",
        "|Computer Vision|image-classification        |                  |Classifies an entire image.                        |Image path, URL, or PIL.Image        |[{'score': float, 'label': str}]                             |\n",
        "|Computer Vision|object-detection            |                  |Detects objects with bounding boxes in an image.   |Image path, URL, or PIL.Image        |                                                           |\n",
        "|Computer Vision|image-segmentation          |                  |Classifies each pixel in an image into a category. |Image path, URL, or PIL.Image        |[{'score': float, 'label': str, 'mask': PIL.Image}]          |\n",
        "|Computer Vision|image-to-text               |                  |Generates a caption for an image.                  |Image path, URL, or PIL.Image        |[{'generated_text': str}]                                    |\n",
        "|Audio          |audio-classification        |                  |Classifies an audio snippet.                       |Audio path, URL, or raw waveform     |[{'score': float, 'label': str}]                             |\n",
        "|Audio          |automatic-speech-recognition|                  |Transcribes speech from an audio file to text.     |Audio path, URL, or raw waveform     |{'text': str}                                                |\n",
        "|Audio          |text-to-speech              |                  |Converts text into a spoken audio waveform.        |str                                  |np.ndarray (waveform), int (sampling rate)                   |\n",
        "|Multimodal     |document-question-answering |                  |Answers a question based on an image of a document.|image, question                      |[{'score': float, 'answer': str,...}]                        |\n",
        "|Multimodal     |visual-question-answering   |                  |Answers a question based on a general image.       |image, question                      |[{'score': float, 'answer': str}]                            |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Beyond the Defaults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When a pipeline is instantiated with only a task identifier, it loads a default, general-purpose model for that task. For example,  `pipeline(\"sentiment-analysis\")` might load a model like `distilbert-base-uncased-finetuned-sst-2-english`. While competent, this model may not be the best choice for text from a specific domain (e.g., legal documents, scientific papers) or in a language other than English.\n",
        "\n",
        "For superior performance, it is often necessary to select a model from the Hugging Face Hub that has been specifically fine-tuned on a more relevant dataset. This is achieved by passing the model's Hub identifier to the  \u00a0`model` parameter during pipeline initialization - the one that we saw on the Models Hub.\n",
        "\n",
        "For instance, to perform sentiment analysis on multilingual product reviews, one could select a model trained for that specific purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9acaff812e3848feb852023e3e7336de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a31b0a1a87e4419ae013ba9ed81ecf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e33ebe186a54a6fb0423f5d52f3cd31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e77101052e94905962604fe48d120dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff91facf94de41bfa538b29fe6c0523e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': '5 stars', 'score': 0.9075827598571777}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Using a model fine-tuned on multilingual reviews\n",
        "multi_lang_classifier = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        ")\n",
        "multi_lang_classifier(\"Ce produit est absolument fantastique!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, if an application requires high accuracy and generalization across diverse English text formats like tweets and reviews, a model like `siebert`/`sentiment-roberta-large-english` might be a better choice due to its training on 15 different datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c8d5e716bd0478a87a60a8fd04fb436"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "219761efda224a19bff2abbb64d52de6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d34e846d570147158345415cf07c13ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57815fc1a32442a8860d8b462b92fa94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "534b9dea75254c24bce0fde1b1f5b181"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "788d2edd65ae4629bbe112c3342bb734"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b34abd7360b4a189a20e569c80317f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9987836480140686}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Using a model designed for high generalization\n",
        "roberta_classifier = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"siebert/sentiment-roberta-large-english\"\n",
        ")\n",
        "roberta_classifier(\"This movie was a masterpiece, but the book was even better.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "gpu_generator = pipeline(\"text-generation\", model=\"gpt2\", device=0)"
      ],
      "metadata": {},
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In most cases, the pipeline correctly infers the appropriate tokenizer to use from the specified model. However, in advanced scenarios, such as when using a `model` and `tokenizer` that are not stored under the same identifier on the Hub, one can explicitly specify the tokenizer using the tokenizer parameter. This ensures that the text pre-processing is perfectly aligned with the model's expectations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3328d5450bad40e2a4f21c7da848ad28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f4402a72e2c48aeb597440a05b8df4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0253074d67ec43cb85df29dab0142693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05dd291b178c40d89b75e7d25c8df358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b833c0483e6e4f28b442edef98db551f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Explicitly specifying both model and tokenizer\n",
        "feature_extractor = pipeline(\n",
        "    \"feature-extraction\",\n",
        "    model=\"distilbert-base-uncased\",\n",
        "    tokenizer=\"distilbert-base-uncased\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hardware Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `device` parameter allows for explicit placement of the model onto a specific hardware device.\n",
        "\n",
        "`device=-1` : This is the default setting, which allocates the model to the CPU.\n",
        "\n",
        "`device=0` : This places the model on the first available CUDA-enabled GPU (identified as CUDA device 0). If you have multiple GPUs, you can use `device=1`, `device=2`, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# Running a text generation pipeline on the first GPU\n",
        "gpu_generator = pipeline(\"text-generation\", model=\"gpt2\", device=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Most common and useful task-specific parameters:\n",
        "\n",
        "For Text Generation (`text-generation`):\n",
        "- `max_length` (int): Sets the maximum length of the generated sequence, including the prompt.\n",
        "- `num_return_sequences` (int): The number of different sequences to generate.\n",
        "- `do_sample` (bool): If True, generation is done via sampling, which introduces randomness and creativity. If False, it uses greedy decoding, which can be repetitive.\n",
        "- `top_k` (int): Restricts sampling to the k most likely next tokens.\n",
        "- `top_p` (float): Nucleus sampling, which restricts sampling to a cumulative probability mass of p.\n",
        "\n",
        "For Classification Tasks (`text-classification`, `zero-shot-classification`):\n",
        "- `top_k` (int): Instead of returning only the single most likely class, it returns the top k predictions.\n",
        "- `return_all_scores` (bool): If True, the pipeline returns a score for every possible label in the model's configuration, not just the predicted one.\n",
        "\n",
        "For Question Answering (`question-answering`):\n",
        "- `top_k` (int): Returns the top k most likely answer spans found in the context text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The AutoTokenizer Family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "accb40022afe44b2a0fcfb8c448d01dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "731fc675628444c8966dfa57c3f95c5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82c9ad65eda446f09eb11096854d9d72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f63ada84f6d4aeabbba238a74f4de1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the fast version of the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "\n",
        "print(type(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the \"Fast\" at the end of the tokenizer type. The Fast AutoTokenizers are built in Rust, and are the default version now.\n",
        "\n",
        "\n",
        "\n",
        "Older versions of AutoTokenizers are also available. These are implemented in Python, and so slow that they are literally called \"Slow\" Tokenizers.\n",
        "\n",
        "\n",
        "Although not recommended, you can explicitly request a slow tokenizer by setting the `use_fast=False` parameter. This may be required for legacy code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cce9f3ffa70544088b8cb9a71df7f494"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbaceba637904e55a81e2f3e6a0dfa26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d2f73c7ae664d15be4bf9da94238d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "764ef63ed3b84f25bbf08c7306fab3a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the slow version of the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=False)\n",
        "\n",
        "print(type(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93c2ece9f36f47caa693ce92d8b07c06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67592aa0679942efb986790d825972fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29421d13bcd54029939386536d74d333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2023, 2607, 2026, 2878, 2166, 1012,\n",
            "          102],\n",
            "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,\n",
            "            0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"I've been waiting this course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "input_ids = tokenizer(raw_inputs, padding=True, return_tensors=\"pt\")\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**`model = \"distilbert-base-uncased-finetuned-sst-2-english\"`**\n",
        "\n",
        "This line defines the pre-trained model we want to use. Let's decode this name:\n",
        "\n",
        "  * **`distilbert-base-uncased`**: This is the base model, a smaller, faster version of BERT. \"Uncased\" means all text was converted to lowercase during its training.\n",
        "  * **`finetuned-sst-2-english`**: This indicates the model was further trained (fine-tuned) on the **SST-2 (Stanford Sentiment Treebank v2)** dataset, which is a collection of English movie reviews labeled with positive or negative sentiment. This fine-tuning makes the model particularly good at sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "**`input_ids = tokenizer(...)`**\n",
        "This is the core step where the tokenizer processes the `raw_inputs`.\n",
        "\n",
        "* **`tokenizer(raw_inputs, ...)`**: The tokenizer takes our list of sentences and converts each one into a sequence of numbers (IDs). Each ID corresponds to a specific \"token\" (a word or part of a word) in the tokenizer's vocabulary.\n",
        "* **`padding=True`**: Since sentences have different lengths, this option pads the shorter sentence with a special padding token ID (`0`) to make both sequences the same length. This is necessary for processing them together as a batch.\n",
        "* **`return_tensors=\"pt\"`**: This specifies that the output should be a PyTorch tensor (`pt`), which is the standard data structure used in the PyTorch deep learning framework.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The Output Explained:**\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "'input_ids': tensor([\n",
        "  [ 101, 1045, 1005, 2310, 2042, 3403, 2023, 2607, 2026, 2878, 2166, 1012,  102],\n",
        "  [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,    0,    0]\n",
        "]),\n",
        "'attention_mask': tensor([\n",
        "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
        "])\n",
        "}\n",
        "```\n",
        "\n",
        "* **`input_ids`**: This tensor contains the numerical representations of your sentences.\n",
        "\n",
        "    * `[101]` is the ID for the special `[CLS]` (classification) token, which is added to the beginning of every sequence.\n",
        "    * `[102]` is the ID for the special `[SEP]` (separator) token, marking the end of a sentence.\n",
        "    * Notice the `0`s at the end of the second sequence. These are the padding tokens added to make its length equal to the first sequence's length.\n",
        "\n",
        "* **`attention_mask`**: This tells the model which tokens to pay attention to.\n",
        "\n",
        "    * It contains a `1` for every real token (including `[CLS]` and `[SEP]`).\n",
        "    * It contains a `0` for every padding token. This ensures the model ignores the padding and doesn't let it influence the final prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The AutoModel Family"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoModel\n",
        "\n",
        "The base `AutoModel` is used to load the core architecture of a pretrained model, which gives you the hidden states (or last hidden state) of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68d9d2c153984489934c6fccf2063f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "125bfd4a151a4a98bbab39b23675e466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48af6175d8f94283b1a788dcfad135d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac262b2a110647259ca51013652a3440"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccb6231a6b6b49f68861907fcf575e04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n",
            "==================================================\n",
            "Logits: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1993, -0.2101, -0.1950,  ..., -0.4733,  0.0861,  0.7103],\n",
            "         [-0.5400, -0.7178, -0.2873,  ..., -0.7211,  0.5801,  0.3946],\n",
            "         [-0.1421, -0.7375,  0.3737,  ..., -0.3740,  0.0750,  0.9687],\n",
            "         ...,\n",
            "         [ 0.1321, -0.2893, -0.0043,  ..., -0.1772, -0.2123, -0.1983],\n",
            "         [ 0.4060,  0.0366, -0.7327,  ...,  0.4169, -0.3416, -0.4542],\n",
            "         [ 0.0646, -0.2088, -0.1323,  ...,  0.5954, -1.0679,  0.0173]]]), pooler_output=tensor([[-9.4299e-01, -5.7258e-01, -8.9093e-01,  8.4917e-01,  7.3438e-01,\n",
            "         -3.1424e-01,  9.3282e-01,  4.1172e-01, -8.2343e-01, -1.0000e+00,\n",
            "         -3.7284e-01,  9.3451e-01,  9.8351e-01,  5.0457e-01,  9.4685e-01,\n",
            "         -7.9498e-01, -3.7354e-01, -6.6457e-01,  4.4397e-01, -7.5095e-01,\n",
            "          7.2463e-01,  9.9999e-01,  3.0325e-01,  4.2836e-01,  5.6970e-01,\n",
            "          9.8481e-01, -7.9837e-01,  9.3837e-01,  9.6563e-01,  7.7279e-01,\n",
            "         -8.2288e-01,  2.7461e-01, -9.9091e-01, -3.3512e-01, -9.2995e-01,\n",
            "         -9.9428e-01,  5.2797e-01, -7.9985e-01, -1.5079e-01, -9.3853e-02,\n",
            "         -9.2339e-01,  4.5827e-01,  9.9999e-01, -3.2961e-01,  4.7323e-01,\n",
            "         -4.3278e-01, -1.0000e+00,  4.0610e-01, -9.1698e-01,  9.0670e-01,\n",
            "          8.8102e-01,  8.5099e-01,  3.3882e-01,  6.2175e-01,  6.2976e-01,\n",
            "         -2.9181e-01,  4.4824e-02,  2.9721e-01, -3.6886e-01, -7.0939e-01,\n",
            "         -7.0921e-01,  4.3761e-01, -7.8405e-01, -9.4754e-01,  8.6314e-01,\n",
            "          8.1323e-01, -2.7983e-01, -4.5793e-01, -3.0205e-01,  3.4432e-02,\n",
            "          9.3495e-01,  4.0481e-01, -2.2344e-01, -8.3385e-01,  7.0226e-01,\n",
            "          4.0168e-01, -6.3696e-01,  1.0000e+00, -6.1509e-01, -9.8158e-01,\n",
            "          8.3299e-01,  7.7479e-01,  6.1630e-01, -3.5069e-01,  5.9604e-01,\n",
            "         -1.0000e+00,  6.9729e-01, -2.4506e-01, -9.8932e-01,  2.3893e-01,\n",
            "          6.4326e-01, -3.9268e-01,  7.6987e-01,  6.5098e-01, -6.7676e-01,\n",
            "         -5.1638e-01, -4.9274e-01, -8.3622e-01, -3.5879e-01, -4.4271e-01,\n",
            "          2.2667e-01, -4.4745e-01, -5.1476e-01, -4.5929e-01,  4.5641e-01,\n",
            "         -5.8722e-01, -5.5043e-01,  6.7926e-01,  1.6857e-01,  8.1656e-01,\n",
            "          4.8666e-01, -4.8503e-01,  5.6523e-01, -9.6008e-01,  6.9900e-01,\n",
            "         -5.0323e-01, -9.8866e-01, -6.2076e-01, -9.8676e-01,  7.9322e-01,\n",
            "         -2.0862e-01, -4.1675e-01,  9.7124e-01, -2.9989e-01,  4.9019e-01,\n",
            "         -2.9001e-01, -9.1348e-01, -1.0000e+00, -5.8614e-01, -6.6061e-01,\n",
            "         -2.6694e-01, -3.9146e-01, -9.8190e-01, -9.6048e-01,  7.5962e-01,\n",
            "          9.7284e-01,  3.3958e-01,  9.9994e-01, -4.4164e-01,  9.5136e-01,\n",
            "         -3.0791e-01, -6.8793e-01,  4.4535e-01, -5.8097e-01,  8.1467e-01,\n",
            "          5.7327e-01, -7.9696e-01,  2.9037e-01, -2.9141e-01,  4.3167e-01,\n",
            "         -7.1466e-01, -4.1172e-01, -8.3761e-01, -9.2677e-01, -4.8236e-01,\n",
            "          9.4554e-01, -5.3289e-01, -9.3398e-01,  8.6826e-02, -3.6185e-01,\n",
            "         -4.9015e-01,  8.9939e-01,  7.6645e-01,  4.9538e-01, -3.7162e-01,\n",
            "          5.9488e-01,  5.5546e-01,  6.5237e-01, -9.2901e-01,  4.2131e-02,\n",
            "          5.8164e-01, -3.5248e-01, -8.7799e-01, -9.8223e-01, -4.9238e-01,\n",
            "          6.1011e-01,  9.8879e-01,  8.1970e-01,  4.4426e-01,  8.2262e-01,\n",
            "         -4.9194e-01,  7.2638e-01, -9.5643e-01,  9.8360e-01, -2.9188e-01,\n",
            "          3.6624e-01, -5.2040e-01,  5.7776e-01, -9.1527e-01,  2.4354e-01,\n",
            "          9.0106e-01, -6.0003e-01, -8.9684e-01, -2.3443e-01, -6.2698e-01,\n",
            "         -5.5630e-01, -8.0445e-01,  6.1571e-01, -4.9513e-01, -4.7420e-01,\n",
            "         -2.6719e-01,  9.3109e-01,  9.8936e-01,  7.7224e-01,  2.3901e-01,\n",
            "          6.7729e-01, -9.1213e-01, -5.6562e-01,  2.0625e-01,  3.9076e-01,\n",
            "          2.5318e-01,  9.9405e-01, -7.1795e-01, -2.9662e-01, -9.5165e-01,\n",
            "         -9.8491e-01,  3.8105e-02, -9.0007e-01, -2.4137e-01, -7.7969e-01,\n",
            "          7.3353e-01, -4.1894e-01,  5.5199e-01,  4.8871e-01, -9.9362e-01,\n",
            "         -8.8404e-01,  4.4750e-01, -5.5172e-01,  6.1803e-01, -3.7621e-01,\n",
            "          8.7174e-01,  9.5399e-01, -7.4851e-01,  7.8246e-01,  9.4368e-01,\n",
            "         -8.7716e-01, -8.1415e-01,  8.9846e-01, -4.2138e-01,  9.3926e-01,\n",
            "         -7.6084e-01,  9.9612e-01,  9.0972e-01,  7.6524e-01, -9.3710e-01,\n",
            "         -7.6951e-01, -9.4897e-01, -7.7909e-01, -2.8920e-01,  1.7346e-01,\n",
            "          8.8910e-01,  7.0076e-01,  4.6642e-01,  1.5714e-01, -7.0514e-01,\n",
            "          9.9919e-01, -8.7127e-01, -9.5810e-01, -2.8223e-01, -2.4028e-01,\n",
            "         -9.9094e-01,  8.8652e-01,  3.2522e-01,  3.7287e-01, -6.2558e-01,\n",
            "         -7.2060e-01, -9.6744e-01,  9.1776e-01,  2.1106e-01,  9.9480e-01,\n",
            "         -5.2141e-01, -9.4396e-01, -7.4704e-01, -9.2990e-01, -3.6372e-02,\n",
            "         -4.2623e-01, -3.1322e-01, -7.3923e-05, -9.5495e-01,  6.3036e-01,\n",
            "          6.3409e-01,  5.8571e-01, -8.5348e-01,  9.9941e-01,  1.0000e+00,\n",
            "          9.7777e-01,  8.8965e-01,  9.3697e-01, -9.9989e-01, -6.0251e-01,\n",
            "          1.0000e+00, -9.9356e-01, -1.0000e+00, -9.5697e-01, -7.7454e-01,\n",
            "          5.0010e-01, -1.0000e+00, -3.4364e-01, -9.9068e-02, -9.1489e-01,\n",
            "          6.7227e-01,  9.8295e-01,  9.9672e-01, -1.0000e+00,  8.9718e-01,\n",
            "          9.4504e-01, -6.8782e-01,  9.3593e-01, -5.7071e-01,  9.7793e-01,\n",
            "          7.2656e-01,  5.8319e-01, -3.5341e-01,  4.9573e-01, -9.3586e-01,\n",
            "         -9.0961e-01, -6.7199e-01, -7.4322e-01,  9.9877e-01,  2.4389e-01,\n",
            "         -8.3662e-01, -9.2477e-01,  5.6935e-01, -1.2380e-01,  3.1797e-02,\n",
            "         -9.6293e-01, -3.4256e-01,  4.8209e-01,  8.4231e-01,  4.0033e-01,\n",
            "          4.0712e-01, -8.0011e-01,  4.7251e-01,  2.3256e-01,  5.1941e-01,\n",
            "          6.8988e-01, -9.4960e-01, -6.5766e-01, -1.7878e-01, -2.0912e-01,\n",
            "         -6.3883e-01, -9.7535e-01,  9.7274e-01, -4.9717e-01,  8.7959e-01,\n",
            "          1.0000e+00,  3.7092e-01, -8.9917e-01,  6.6480e-01,  3.5943e-01,\n",
            "         -6.4960e-01,  1.0000e+00,  7.9131e-01, -9.8566e-01, -6.1673e-01,\n",
            "          7.1090e-01, -6.9198e-01, -7.0822e-01,  9.9973e-01, -4.4506e-01,\n",
            "         -7.3371e-01, -4.7340e-01,  9.7718e-01, -9.9224e-01,  9.9549e-01,\n",
            "         -9.1442e-01, -9.6908e-01,  9.6958e-01,  9.4492e-01, -6.3145e-01,\n",
            "         -8.0897e-01,  2.5237e-01, -8.2719e-01,  4.2726e-01, -9.7301e-01,\n",
            "          7.9668e-01,  5.4817e-01, -2.1784e-01,  8.9414e-01, -8.9280e-01,\n",
            "         -6.4768e-01,  5.4165e-01, -6.1155e-01, -6.6730e-03,  9.5998e-01,\n",
            "          5.9635e-01, -3.9007e-01,  1.6957e-01, -4.0874e-01, -5.7668e-01,\n",
            "         -9.7191e-01,  5.3368e-01,  1.0000e+00, -2.8201e-01,  6.8223e-01,\n",
            "         -4.9309e-01, -2.0582e-01,  5.5119e-02,  6.6335e-01,  7.1513e-01,\n",
            "         -4.2704e-01, -9.0475e-01,  8.5073e-01, -9.7213e-01, -9.8840e-01,\n",
            "          7.9919e-01,  3.4014e-01, -4.5839e-01,  1.0000e+00,  6.0477e-01,\n",
            "          2.5195e-01,  4.7606e-01,  9.8021e-01,  1.7525e-01,  6.0231e-01,\n",
            "          8.4835e-01,  9.8093e-01, -4.2150e-01,  6.1582e-01,  9.0732e-01,\n",
            "         -9.0096e-01, -4.5042e-01, -7.9273e-01,  6.5279e-02, -9.5087e-01,\n",
            "         -1.1438e-01, -9.6163e-01,  9.7313e-01,  9.0516e-01,  4.9724e-01,\n",
            "          3.9209e-01,  7.8612e-01,  1.0000e+00, -3.4641e-01,  7.3736e-01,\n",
            "         -4.8711e-01,  8.8986e-01, -9.9986e-01, -8.9589e-01, -5.2291e-01,\n",
            "         -2.1567e-01, -7.5208e-01, -4.3104e-01,  4.1335e-01, -9.6966e-01,\n",
            "          7.9161e-01,  6.4767e-01, -9.9344e-01, -9.8960e-01, -2.4173e-01,\n",
            "          8.8211e-01,  1.5753e-01, -9.8476e-01, -7.1690e-01, -6.7585e-01,\n",
            "          6.9141e-01, -4.7022e-01, -9.2647e-01, -9.9640e-02, -4.9025e-01,\n",
            "          5.6206e-01, -4.6670e-01,  6.3577e-01,  8.8391e-01,  5.7393e-01,\n",
            "         -8.5811e-01, -2.5894e-01, -1.9639e-01, -8.4241e-01,  8.6346e-01,\n",
            "         -8.8620e-01, -9.1561e-01, -2.6486e-01,  1.0000e+00, -7.0048e-01,\n",
            "          9.2459e-01,  8.0708e-01,  8.0524e-01, -3.4788e-01,  2.5535e-01,\n",
            "          9.4768e-01,  3.5077e-01, -8.0590e-01, -8.5887e-01, -7.5780e-01,\n",
            "         -4.8603e-01,  7.4089e-01,  5.4085e-01,  8.1245e-01,  8.4715e-01,\n",
            "          8.3696e-01,  1.8781e-01, -1.4961e-01,  2.1405e-01,  9.9985e-01,\n",
            "         -2.5312e-01, -4.2248e-01, -7.1812e-01, -2.2723e-01, -4.5109e-01,\n",
            "         -2.9949e-01,  1.0000e+00,  4.8099e-01,  3.6256e-01, -9.9149e-01,\n",
            "         -8.7700e-01, -9.4159e-01,  1.0000e+00,  8.3847e-01, -8.7695e-01,\n",
            "          8.0982e-01,  5.4304e-01, -2.5525e-01,  8.1958e-01, -3.9162e-01,\n",
            "         -3.6645e-01,  2.9118e-01,  2.3498e-01,  9.5400e-01, -6.7039e-01,\n",
            "         -9.6634e-01, -7.2011e-01,  5.4097e-01, -9.6847e-01,  9.9994e-01,\n",
            "         -7.7072e-01, -3.9436e-01, -5.1865e-01, -2.1231e-01,  6.9002e-01,\n",
            "          9.5955e-02, -9.8076e-01, -3.0263e-01,  2.3811e-01,  9.7268e-01,\n",
            "          4.0723e-01, -6.2987e-01, -9.3956e-01,  8.2620e-01,  7.7330e-01,\n",
            "         -8.5822e-01, -9.3199e-01,  9.6850e-01, -9.8046e-01,  6.2411e-01,\n",
            "          1.0000e+00,  5.4706e-01, -6.8841e-02,  3.8605e-01, -6.5183e-01,\n",
            "          4.6342e-01, -2.8666e-01,  7.6140e-01, -9.6358e-01, -5.2746e-01,\n",
            "         -4.1265e-01,  4.5434e-01, -3.3552e-01, -2.6719e-01,  6.9706e-01,\n",
            "          2.9986e-01, -5.6544e-01, -6.8939e-01, -3.4395e-01,  5.4504e-01,\n",
            "          8.8369e-01, -3.2734e-01, -3.1823e-01,  1.9555e-01, -3.3232e-01,\n",
            "         -9.2918e-01, -4.7741e-01, -6.0873e-01, -1.0000e+00,  7.5087e-01,\n",
            "         -1.0000e+00,  5.7610e-01,  2.0790e-01, -3.1736e-01,  8.7145e-01,\n",
            "          6.0871e-01,  7.7120e-01, -8.0965e-01, -8.1214e-01,  3.5827e-01,\n",
            "          7.9765e-01, -4.7965e-01, -2.8905e-01, -7.5634e-01,  4.7069e-01,\n",
            "         -2.3533e-01,  3.8380e-01, -6.1306e-01,  8.2071e-01, -3.8124e-01,\n",
            "          1.0000e+00,  2.4438e-01, -7.6696e-01, -9.8557e-01,  3.9646e-01,\n",
            "         -4.1885e-01,  1.0000e+00, -9.6038e-01, -9.6126e-01,  4.5108e-01,\n",
            "         -8.3281e-01, -8.5607e-01,  4.6982e-01,  2.2934e-01, -8.1769e-01,\n",
            "         -9.6042e-01,  9.6643e-01,  9.3020e-01, -6.0056e-01,  5.8753e-01,\n",
            "         -4.2803e-01, -6.4833e-01,  1.1353e-01,  8.4492e-01,  9.8801e-01,\n",
            "          6.4064e-01,  9.5517e-01,  1.1137e-01, -4.4765e-01,  9.7652e-01,\n",
            "          4.2299e-01,  6.4696e-01,  2.1821e-01,  1.0000e+00,  4.4588e-01,\n",
            "         -9.5404e-01,  1.0843e-01, -9.9031e-01, -3.7959e-01, -9.7207e-01,\n",
            "          4.1987e-01,  4.1573e-01,  9.1830e-01, -4.3177e-01,  9.7062e-01,\n",
            "         -8.1857e-01,  1.9719e-01, -5.3266e-01, -4.2440e-01,  4.6422e-01,\n",
            "         -9.3207e-01, -9.8856e-01, -9.8420e-01,  6.8208e-01, -5.7221e-01,\n",
            "         -2.4689e-01,  3.8592e-01,  2.5520e-01,  5.8242e-01,  5.0715e-01,\n",
            "         -1.0000e+00,  9.4845e-01,  5.9428e-01,  9.1268e-01,  9.5631e-01,\n",
            "          6.2173e-01,  6.0834e-01,  3.5926e-01, -9.8728e-01, -9.8819e-01,\n",
            "         -5.2150e-01, -4.8239e-01,  8.3559e-01,  7.1770e-01,  9.1170e-01,\n",
            "          5.0261e-01, -5.6209e-01, -6.2910e-01, -6.3388e-01, -7.2700e-01,\n",
            "         -9.9325e-01,  5.2566e-01, -6.3572e-01, -9.7949e-01,  9.6491e-01,\n",
            "          1.0157e-01, -2.8965e-01, -2.5126e-02, -8.1038e-01,  9.5513e-01,\n",
            "          8.6251e-01,  5.0891e-01,  1.3635e-01,  6.1807e-01,  9.0050e-01,\n",
            "          9.7696e-01,  9.8731e-01, -8.3567e-01,  8.6648e-01, -5.7461e-01,\n",
            "          6.4811e-01,  6.4998e-01, -9.4729e-01,  3.6252e-01,  5.9195e-01,\n",
            "         -4.9330e-01,  3.7550e-01, -3.8633e-01, -9.8395e-01,  6.3594e-01,\n",
            "         -3.8721e-01,  6.1967e-01, -5.4359e-01, -5.8517e-02, -5.3679e-01,\n",
            "         -3.5283e-01, -8.6642e-01, -7.6552e-01,  6.6115e-01,  5.5424e-01,\n",
            "          9.0701e-01,  8.6111e-01, -2.5044e-01, -7.9111e-01, -3.5136e-01,\n",
            "         -8.3013e-01, -9.4113e-01,  9.6203e-01, -1.2759e-01, -4.7110e-01,\n",
            "          7.5167e-01,  1.0877e-01,  8.3754e-01,  2.4697e-01, -5.4928e-01,\n",
            "         -4.2823e-01, -8.6458e-01,  9.1330e-01, -5.3473e-01, -6.7663e-01,\n",
            "         -6.3501e-01,  7.1364e-01,  4.4918e-01,  9.9999e-01, -8.1240e-01,\n",
            "         -8.7004e-01, -4.4191e-01, -4.5436e-01,  5.2397e-01, -5.3613e-01,\n",
            "         -1.0000e+00,  5.6194e-01, -4.9146e-01,  7.2476e-01, -6.3816e-01,\n",
            "          7.8825e-01, -4.8224e-01, -9.8791e-01, -3.1777e-01,  6.9717e-01,\n",
            "          7.2913e-01, -6.2339e-01, -6.3784e-01,  6.0367e-01,  5.6177e-02,\n",
            "          9.6964e-01,  9.1225e-01, -4.5676e-01, -1.8579e-02,  6.4981e-01,\n",
            "         -8.6359e-01, -7.3628e-01,  9.3377e-01]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# Prepare the input text\n",
        "text = \"This is a sample sentence.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Get the model's last hidden state\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(last_hidden_states.shape)\n",
        "print(\"=\" * 50)\n",
        "print(f\"Logits: {outputs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoModelForSequenceClassification\n",
        "\n",
        "This variant is designed for text classification tasks. It includes a classification head on top of the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class ID: 1\n",
            "Predicted label: POSITIVE\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Prepare the input text\n",
        "text = \"Hugging Face is doing a great job.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Get the classification logits\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "print(f\"Predicted class ID: {predicted_class_id}\")\n",
        "print(f\"Predicted label: {model.config.id2label[predicted_class_id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoModelForCausalLM\n",
        "\n",
        "This is used for causal language modeling, which is essentially text generation. It has a language modeling head that predicts the next token in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5678378ad9164bf6be093f9664087f2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0e3664ca4324de289d5e92321c22032"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddefc67985c04d9aa23776ec11533dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e2a0206539241e5a5f2ff9e27106396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d32057bfcf954793bf55654f9cff6c56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d9b600e53e4d49879153f9ccb592ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76b7d9839daa464ba01f64cbb5221e97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox jumps over the fence and runs into the bushes.\n",
            "\n",
            "\"I'm going\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the tokenizer and model for causal language modeling\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# Prepare the input text\n",
        "text = \"The quick brown fox jumps over the\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "outputs = model.generate(**inputs, max_length=20)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoModelForQuestionAnswering\n",
        "\n",
        "This variant is designed for extractive question answering, where the model finds the answer to a question within a given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "737d6d5113e749d39527211216f79385"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c8b8ff5003044c8a1ff7fd5bdb9fddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4013644f92ae40619d990b991fd45e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37c602a79aae4599a6118a5df3b3d9cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "995507de3b85449db7179c4f1c5acc4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e85475b066e481f9a363c86fa2e4ce0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  Paris\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Define the question and context\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"France is a country in Western Europe. Its capital and largest city is Paris.\"\n",
        "\n",
        "# Prepare the inputs\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "\n",
        "# Get the start and end logits for the answer\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Find the tokens with the highest start and end scores\n",
        "answer_start_index = torch.argmax(outputs.start_logits)\n",
        "answer_end_index = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "# Convert the token indices to the answer string\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index]\n",
        "answer = tokenizer.decode(predict_answer_tokens)\n",
        "\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some parameters to be aware of\n",
        "\n",
        "|Parameter                    |Description                                                                                                                             |Example                            |\n",
        "|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------|\n",
        "|pretrained_model_name_or_path|The identifier for the model. Can be a model ID from the Hub or a local path.                                                           |\"google-bert/bert-base-uncased\"    |\n",
        "|cache_dir                    |Path to a directory to cache downloaded models, overriding the default.                                                                 |cache_dir=\"/path/to/my/cache/\"     |\n",
        "|torch_dtype                  |Overrides the default data type to load the model with a different precision (e.g., torch.float16). Essential for reducing memory usage.|torch_dtype=torch.bfloat16         |\n",
        "|device_map                   |Specifies how to distribute model layers across devices (GPUs, CPU, disk). Set to \"auto\" for automatic distribution.                    |device_map=\"auto\"                  |\n",
        "|max_memory                   |A dictionary specifying the max memory to allocate on each device when using device_map=\"auto\".                                         |max_memory={0: \"12GiB\", 1: \"16GiB\"}|\n",
        "|low_cpu_mem_usage            |If True, avoids fully loading the model into CPU RAM before distributing it, reducing peak memory usage.                                |low_cpu_mem_usage=True             |\n",
        "|trust_remote_code            |If True, allows execution of custom Python code from the model's repository. Use with caution.                                          |trust_remote_code=True             |\n",
        "|revision                     |Specifies a particular model version to load, using a Git branch, tag, or commit hash. Ensures reproducibility.                         |revision=\"v1.0.0\"                  |\n",
        "|local_files_only             |If True, only looks at local files and does not attempt to download from the Hub.                                                       |local_files_only=True              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Hugging Face Datasets Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'datasets' from '/usr/local/lib/python3.11/dist-packages/datasets/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install -U datasets -q\n",
        "import importlib\n",
        "import datasets\n",
        "\n",
        "importlib.reload(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets\n",
        "dataset = load_dataset(\"stanfordnlp/imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at what each part means:\n",
        "- **DatasetDict:** This is the main container, similar to a Python dictionary, where the keys are the names of the dataset splits (like 'train', 'test') and the values are the Dataset objects themselves.\n",
        "  - **train:** This is the training set. It's a Dataset object containing 25,000 rows of data that you would use to teach or \"train\" your machine learning model.\n",
        "  - **test:** This is the testing set. It's another Dataset object with 25,000 rows. You use this data to evaluate your model's performance after it has been trained.\n",
        "  - **unsupervised:** This split contains 50,000 rows of data. This data is unlabeled, meaning it has the same 'text' and 'label' structure but the 'label' column is empty.\n",
        "\n",
        "Every split follows the following structure:\n",
        "- **features:**['text', 'label']: This tells you each row in the dataset has two columns or features: a text column ( containing sentences or documents) and a label column (which holds the class for the corresponding text).\n",
        "- **num_rows:** The total number of examples or data points available in that specific split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can get a split by using square brackets, and, then, get any row of the split by using simple indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use slicing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n",
              "  \"I can't believe that those praising this movie herein aren't thinking of some other film. I was prepared for the possibility that this would be awful, but the script (or lack thereof) makes for a film that's also pointless. On the plus side, the general level of craft on the part of the actors and technical crew is quite competent, but when you've got a sow's ear to work with you can't make a silk purse. Ben G fans should stick with just about any other movie he's been in. Dorothy S fans should stick to Galaxina. Peter B fans should stick to Last Picture Show and Target. Fans of cheap laughs at the expense of those who seem to be asking for it should stick to Peter B's amazingly awful book, Killing of the Unicorn.\",\n",
              "  'Never cast models and Playboy bunnies in your films! Bob Fosse\\'s \"Star 80\" about Dorothy Stratten, of whom Bogdanovich was obsessed enough to have married her SISTER after her murder at the hands of her low-life husband, is a zillion times more interesting than Dorothy herself on the silver screen. Patty Hansen is no actress either..I expected to see some sort of lost masterpiece a la Orson Welles but instead got Audrey Hepburn cavorting in jeans and a god-awful \"poodlesque\" hair-do....Very disappointing....\"Paper Moon\" and \"The Last Picture Show\" I could watch again and again. This clunker I could barely sit through once. This movie was reputedly not released because of the brouhaha surrounding Ms. Stratten\\'s tawdry death; I think the real reason was because it was so bad!',\n",
              "  \"Its not the cast. A finer group of actors, you could not find. Its not the setting. The director is in love with New York City, and by the end of the film, so are we all! Woody Allen could not improve upon what Bogdonovich has done here. If you are going to fall in love, or find love, Manhattan is the place to go. No, the problem with the movie is the script. There is none. The actors fall in love at first sight, words are unnecessary. In the director's own experience in Hollywood that is what happens when they go to work on the set. It is reality to him, and his peers, but it is a fantasy to most of us in the real world. So, in the end, the movie is hollow, and shallow, and message-less.\",\n",
              "  'Today I found \"They All Laughed\" on VHS on sale in a rental. It was a really old and very used VHS, I had no information about this movie, but I liked the references listed on its cover: the names of Peter Bogdanovich, Audrey Hepburn, John Ritter and specially Dorothy Stratten attracted me, the price was very low and I decided to risk and buy it. I searched IMDb, and the User Rating of 6.0 was an excellent reference. I looked in \"Mick Martin & Marsha Porter Video & DVD Guide 2003\" and \\x96 wow \\x96 four stars! So, I decided that I could not waste more time and immediately see it. Indeed, I have just finished watching \"They All Laughed\" and I found it a very boring overrated movie. The characters are badly developed, and I spent lots of minutes to understand their roles in the story. The plot is supposed to be funny (private eyes who fall in love for the women they are chasing), but I have not laughed along the whole story. The coincidences, in a huge city like New York, are ridiculous. Ben Gazarra as an attractive and very seductive man, with the women falling for him as if her were a Brad Pitt, Antonio Banderas or George Clooney, is quite ridiculous. In the end, the greater attractions certainly are the presence of the Playboy centerfold and playmate of the year Dorothy Stratten, murdered by her husband pretty after the release of this movie, and whose life was showed in \"Star 80\" and \"Death of a Centerfold: The Dorothy Stratten Story\"; the amazing beauty of the sexy Patti Hansen, the future Mrs. Keith Richards; the always wonderful, even being fifty-two years old, Audrey Hepburn; and the song \"Amigo\", from Roberto Carlos. Although I do not like him, Roberto Carlos has been the most popular Brazilian singer since the end of the 60\\'s and is called by his fans as \"The King\". I will keep this movie in my collection only because of these attractions (manly Dorothy Stratten). My vote is four.<br /><br />Title (Brazil): \"Muito Riso e Muita Alegria\" (\"Many Laughs and Lots of Happiness\")'],\n",
              " 'label': [0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "dataset[\"train\"][10:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you try to get the type of the split, you will a Apache Arrow Dataset.\n",
        "This is because the datasets library is built on top of [Apache Arrow](https://arrow.apache.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>datasets.arrow_dataset.Dataset</b><br/>def __init__(arrow_table: Table, info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None, indices_table: Optional[Table]=None, fingerprint: Optional[str]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py</a>A Dataset backed by an Arrow table.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 628);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "type(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, if you try to get the type of one single row, you will fing that it is the classic python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "type(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The `map` Function\n",
        "\n",
        "The map function allows you to apply a function over all the splits of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b158106ba72463b9c14a1b6df7ddbdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'], 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask'], 'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'attention_mask']}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) # batched=True increases speed\n",
        "print(tokenized_datasets.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other than the map function, you also some common function that can be applied over the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 3668\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
        "tokenized_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`batched=True` processes the dataset by batch. That is, instead of processing row-by-row, this parameter allows us create a batch of say 8 rows, and process the entire batch one at a time.\n",
        "\n",
        "The size of the batch has to match the amount of RAM available in your system. Too little and the processing takes time, too high and your system crashes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standard Padding\n",
        "\n",
        "You pick one fixed max_length (e.g., 128 tokens) and apply it to every sequence in the entire dataset. Every sequence shorter than this length is padded, and every sequence longer is truncated. All resulting data samples have the exact same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf0ae534af2b44f4bd45b36c75a3a9c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce07a0de1d834570b12d19f3cb15a529"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b95d45e80f446d3a7f3938ae6093a14"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n",
            "torch.Size([16, 128])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DataLoader are iterators over the Dataset data type - similar to using .items for dictionary loops\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=16)\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch[\"input_ids\"].shape) # fixed shape\n",
        "  if step > 5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros**\n",
        "\n",
        "* **Simplicity:** It's very straightforward. You define the length once, and you're done.\n",
        "* **Uniformity:** Every single tensor fed to the model will have the exact same shape (e.g., `[16, 128]`). This predictability can be useful for debugging.\n",
        "\n",
        "**Cons**\n",
        "\n",
        "* **Massive Inefficiency:** This is the biggest drawback. If your `max_length` is 512, but a batch contains sequences that are all around 30 tokens long, you are forcing the GPU to process **~94% padding tokens**. This wastes a significant amount of computation and memory, slowing down training.\n",
        "* **Potential Information Loss:** If you choose a `max_length` that is too small, you will permanently truncate data from longer sequences before training even begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Padding\n",
        "\n",
        "The padding length is determined dynamically for each batch. When creating a batch, you find the longest sequence within that specific batch and pad all other sequences in the batch to match its length. This means different batches will have different sequence lengths. This is typically handled by a DataCollator in the Hugging Face ecosystem.\n",
        "\n",
        "It's achieved by leaving the padding argument out of the `.map()` call and using a `DataCollatorWithPadding` in your `DataLoader`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b029648f6dc41ed9289543d694d972f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "004d8cad761b4147a94938942f333427"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a92d123c2b2d4586a4c2c411b49331ac"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"idx\", \"sentence1\", \"sentence2\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 65])\n",
            "torch.Size([16, 74])\n",
            "torch.Size([16, 82])\n",
            "torch.Size([16, 70])\n",
            "torch.Size([16, 82])\n",
            "torch.Size([16, 76])\n",
            "torch.Size([16, 74])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=16, collate_fn=data_collator)\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "  print(batch[\"input_ids\"].shape) # fixed shape\n",
        "  if step > 5:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pros**\n",
        "\n",
        "* **High Efficiency:** This is the primary advantage. By creating batches that are as compact as possible, you drastically reduce the number of padding tokens. This **speeds up training** and **lowers memory usage**, sometimes by a large margin.\n",
        "* **No Unnecessary Computation:** The model only processes the information that's actually there, plus the minimum required padding for that specific batch.\n",
        "\n",
        "**Cons**\n",
        "\n",
        "* **Variable Shapes:** The input tensor shape will change with every batch (e.g., one batch is `[16, 92]`, the next is `[16, 115]`). Modern deep learning frameworks like PyTorch and TensorFlow handle this seamlessly, but it's a point to be aware of.\n",
        "* **Slightly More Complex Setup:** It's not just a single argument. You need to know to omit padding during the mapping phase and then add a `DataCollator` object when creating your `DataLoader`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Summary: Which Should You Use?**\n",
        "\n",
        "| Feature | Standard Padding | Dynamic Padding |\n",
        "| :--- | :--- | :--- |\n",
        "| **Primary Goal** | Simplicity, compatibility | **Training efficiency** |\n",
        "| **Performance** | Slower, more memory | **Faster, less memory** |\n",
        "| **Implementation** | Simpler (one argument) | Requires a `DataCollator` |\n",
        "| **Best For...** | Exporting models to specific environments, debugging | **Almost all training and evaluation tasks** |\n",
        "\n",
        "**Recommendation:** For training models, **always prefer dynamic padding**. The performance gains are significant. Use standard padding only if you have a specific deployment requirement for fixed-size inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}